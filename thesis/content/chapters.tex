%\documentclass[../xlapes02]{subfiles}
%\begin{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Introduction
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Introduction}\label{ch:introduction}

% TODO: Purpose of RL
% TODO: Purpose of MDP
% TODO

% Overview of what this thesis is about
The stock market is a wide and quickly changing environment.
A lot of people are interested in it and a lot of people are trying to make money from it.
How to make money from it in the long or short term?
This thesis will focus on long-term investing, also called Portfolio Allocation~\cite[p.~67--69]{rao-2022}.

% Main goal of this thesis
The goal is to do the right decision at the right time, which in the Stock market means, when and at what price we should buy this stock or another.
This is more complicated than it seems because we are emotionally
based, and not every person is able to make rational decisions.

% RL
Here come into play Artificial Intelligence (AI) and Reinforcement Learning (RL).
Reinforcement Learning (RL) is one of three types of Artificial Intelligence.

RL is learning what to do, how to map situations to actions, to maximize a numerical reward signal.
RL differs from supervised learning in that the agent is not told what to do,
but instead must discover for itself what actions yield the most
reward by trial and error~\cite[p.~1]{sutton-2018}.

Based on this rewards and losses, the agent improves its decision-making
to get more rewards and fewer losses in the future
in interaction with the environment.

% MC & MDP
The relationship between Markov Decision Process (MDP) and RL is
that RL problem could be well described im MDP\@.
And more precisely is problem described in MDP,
the better and faster could RL agent learn.

The base for MDPs is Markov Chain (MC),
which is a stochastic model describing a sequence of possible events in which the probability
of each event depends only on the state attained in the previous event.
MDPs from MCs differ from by adding actions and rewards.
In other words, MDPs become MCs with one action in a states and all rewards are same
for given state and action.

% Dynamic Programming
In Dynamic Programming, we can see two variants of Reinforcement Learning
Policy Iteration and Value Iteration.

% Why do I choose this topic
In my opinion, the stock market is a very interesting topic.
I love investing and what is called ''passive income''.
This topic was already covered by many people, but because of the
vision of getting rich, people tend to keep their solutions secret.

As Warren Buffett said, ''Be fearful when others are greedy and greedy when others are fearful.''

% Goal
The goal is to evaluate, benchmark and try to improve the current solutions of models
based on Reinforcement Learning and MDPs from AI4Finance-Foundation~\cite{https://doi.org/10.48550/arxiv.2111.03995}.
The goal is to make a model that will be able to make decisions in the stock market
in order to maximize the profit by allocating the portfolio.

% TODO: Thesis Structure

\if 0

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Preliminaries
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\chapter{Preliminaries}\label{ch:preliminaries}
TODO


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Reinforcement Learning
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\chapter{Reinforcement Learning}\label{ch:reinforcement-learning}
Reinforcement learning in general are used for decision-making.
In RL there is no supervisor, who teaches the agent what actions to take.

So it is obvious that agent get the result after the action is taken.

The Reinforcement Learning is now used e.q. in robotics, games, finance, etc\ldots


The purpose of this chapter is to introduce the reader to the basics of Reinforcement Learning.

Section \ref{sec:rl-introduction} introduces the reader to the basics of Reinforcement Learning.
Explain the terminology and the basic concepts of Reinforcement Learning.
Section \ref{sec:rl-introduction} introduces the reader to the basics of Reinforcement Learning.
Explain the terminology and the basic concepts of Reinforcement Learning.

A reward $R_t$ is a scalar feedback signal.
Reinforcement Learning is based on \textit{reqard hypothesis} that the agent's goal is to maximize the total reward.

The information which determining the next state are: current state, action, reward

The current state is the function of history.
$S_t = f(H_t)$

Even if envitonemt in current state $S_{t}^{e}$ is visible, it may contain irrelevant information.


\section{Introduction}\label{sec:rl-introduction}

\subsubsection{Reinforcement Learning related to Stock Market}\label{subsec:rl-introduction}
\begin{itemize}
    \item Sometimes the reward could have long term consequences.
    \item Rewards could be delayed.
    \item Sometimes it may be better to sacrifice short-term rewards for long-term gains.
\end{itemize}

\begin{itemize}
    \item observations
    \item actions
    \item rewards
\end{}

\textbf{foo}
\textrm{fff}


\begin{definition}
    \textit{history} is the sequence of observations, actions, and rewards that the agent has experienced.
\end{definition}

\fi


\section{Classical Reinforcement Learning}\label{sec:classical-reinforcement-learning}
TODO


\section{Parts of Reinforcement Learning}\label{sec:parts-of-reinforcement-learning}
TODO


\section{Functionalities of Reinforcement Learning}\label{sec:functionalities-of-reinforcement-learning}
TODO


\section{Reinforcement Learning Algorithms}\label{sec:reinforcement-learning-algorithms2}
TODO


\section{Deep Reinforcement Learning}\label{sec:deep-reinforcement-learning}
TODO

\subsection{Exploration vs. Exploitation}\label{subsec:exploration-vs.-exploitation}
TODO


\section{Reinforcement Learning Algorithms}\label{sec:reinforcement-learning-algorithms}
TODO: Describe used RL Algorithms

\subsection{Deep Reinforcement Learning Algorithms}\label{subsec:deep-reinforcement-learning-algorithms}
TODO

\subsubsection{PPO}
TODO

\subsubsection{SAC}
TODO

\subsubsection{TD3}
TODO

\subsubsection{DDPG}
TODO


\section{Existing solutions}\label{sec:existing-solutions}
TODO


\section{Neural Networks}\label{sec:neural-networks}
TODO


\section{Used Frameworks}\label{sec:used-frameworks}
TODO


\section{Markov Property}
This equation says that the future is independent of the past given the present.
$\probP[S_{t+1}|S_t] = \probP[S_{t+1}|S_1,\ldots,S_t]$


\section{Partially Observable Markov Decision Process (POMDP)}

\subsection{Partially Observable Environments}
It is the situation where the agent have the restricted access to the environment
and information provided by them are available only partially.

Agent must remember everything that happened in the past.
Own agent representation $S_t^d$.
Complete environment representation $S_t^a=H_t$.
Belief of environment state: $S_t^a=(\probP[S_t^e=s^1])$.


\section{Value Function}
Value function is the expected return starting from state $s$ and then following policy $\pi$.
$v_{\pi}(s)=\expectP[R_t+\gamma\,R_{t+1}+\gamma^2\,R_{t+2}+\ldots|S_t=s]$

Transition probability $P_{ss'}^{\pi}$ is the probability of transitioning from state $s$ to state $s'$ under policy $\pi$.

\[
    \mathcal{P}_{ss'}^a=\probP[S'=s'|S=s,A=a]
\]

Reward function $R(s,a,s')$ is the reward received when transitioning from state $s$ to state $s'$ under action $a$.
\[
    R_s^a=\expectP[R|S=s,A=s]
\]

\subsection{Value Based Agents}\label{subsec:value-based-agents}
Value Function only.
Value Function is the expected return starting from state $s$ and then following policy $\pi$.

\subsection{Policy Agents}\label{subsec:policy-agents}
Policy only.
Policy is the probability of taking action $a$ in state $s$.

\subsection{Actor-Critic Agents}\label{subsec:actor-critic-agents}
Policy + Value function


\section{Policy}
Policy is how agent behaves in the environment.
Map state to actions
Deterministic policy $a=\pi(s)$.
$\pi(a|s)=\probP[A=a|S=s]$

\subsection{Model Free}
Policy and/or Value Function.


\section{RL Agent Taxonomy}
% TODO: add image from: https://youtu.be/2pWv7GOvuf0?list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ&t=4549


\section{Sequential Decision Making}
Here two problems need to be solved:
In Reinforcement Learning:
\begin{itemize}
    \item The environment is initially unknown.
    \item The agent must learn to act in the environment.
    \item The agent improves its policy by interacting with the environment.
\end{itemize}

Planning:
\begin{itemize}
    \item A model of the environment is unknown.
    \item The agent performs computations with its model (without any external interaction).
    \item The agent improves its policy by performing computations with its model.
\end{itemize}

The planning ahead is needed to find the optimal policy, e.g. tree search.

All the time we are solving the problem of what will be the next state and what will be the reward.

Reinforcement Learning is like trial-and-error learning.


\section{Exploration vs. Exploitation}\label{sec:exploration-vs.-exploitation}
The time and costs is divided into two parts: Exploration and Exploitation.

The goal is to fidn the best trade-off between exploration and exploitation.

In the~\ref{subsec:exploration} the exploration approach is described and
in the~\ref{subsec:exploitation} the exploitation approach is described.

\subsection{Exploration}\label{subsec:exploration}
Finding new information about the environment.

\subsection{Exploitation}\label{subsec:exploitation}
Exploiting the information that is already known to maximize the reward.


\section{Prediction and Control}\label{sec:prediction-vs.-control}
Here to solve the control problem we need to solve the prediction problem first.
In the~\ref{subsec:exploration} the exploration approach is described and
in the~\ref{subsec:exploitation} the exploitation approach is described.

\subsection{Prediction}\label{subsec:prediction}
Given a policy.

\subsection{Control}\label{subsec:control}
Finding the best policy.


\section{Markov Processes (also called Markov Chains)}\label{sec:markov-processes}
Markov Process is a stochastic process where the future is independent of the past given the present.

\begin{definition}
    A Markov process is a tuple $(S,A,P)$ where:
    \begin{itemize}
        \item $S$ is a finite set of states.
        \item $A$ is a finite set of actions.
        \item $P$ is a transition probability function fot time $t$, $P_t:S\times A\times S\to[0,1]$.
    \end{itemize}
\end{definition}
$\probP[S_{t+1}|S_t] = \probP[S_{t+1}|S_1,\ldots,S_t]$


\section{Markov Reward Process (MRP)}\label{sec:markov-reward-process}
Markov Reward Process is a Markov Process with a reward function.
MRP is a tuple of $g\in G\subseteq L \times \mathrm{Ops} \times L$
\begin{definition}
    A Markov Reward Process is a Markov Process, along with a time-indexed
    sequence of Reward random variables $R_t \in D$ (a countable subset of R) for time steps $t =
    1,2,\ldots$, satisfying the Markov Property (including Rewards):
    $\probP[(R_{t+1}, S_{t+1})|S_t, S_{t-1}, \ldots, S_0] = P[(Rt+1, St+1)|St]$ for all $t \leq 0.$~\cite[p.~79]{rao-2022}
\end{definition}


\section{Markov Decision Process}\label{sec:markov-decision-process}
Formally describe an environment for reinforcement learning.

The current state $S_t$ completely characterizes the process. % TODO: is it correct?
Almost all RL problems can be described as MDPs.
Optimal control primarily deals with continous MDPs.
Into MSPs the POMDPs could be converted.

\[
    \probP[S_{t+1}|S_t]=\probP[S_{t+1}|S_1,\ldots,S_t]
\]

State transition matrix.
\[
    \mathcal{P}_{ss'}^a=\probP[S'=s'|S=s,A=a]
\]

where $S'$ is the all next possible states and $s'$ is the next concrete state from $S'$.

\[
    \mathcal{P} = \begin{bmatrix}
                      \mathcal{P}_{11} & \dots  & \mathcal{P}_{1n} \\
                      \vdots           & \ddots & \vdots           \\
                      \mathcal{P}_{11} & \dots  & \mathcal{P}_{nn}
    \end{bmatrix}
\]
where each row of the matrix sum to 1.

\subsection{Bellman Equation}\label{subsec:bellman-equation}
TODO

\subsection{Bellman Optimality Equation}\label{subsec:bellman-optimality-equation}
TODO


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Exisiting Approaches
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\chapter{Existing Approaches to Portfolio Allocation}\label{ch:existing-approaches-to-portfolio-allocation}
TODO


\section{Introduction}\label{sec:introduction}
TODO


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Environment
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\chapter{Environment}\label{ch:environment}
TODO


\section{Stock Market Environment}\label{sec:stock-market-environment}
TODO

\subsection{Used Frameworks}\label{subsec:used-frameworks}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Data
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\chapter{Data Engineering}\label{ch:data-engineering}
TODO


\section{Data Collection}\label{sec:data-collection}
TODO


\section{Data Preprocessing}\label{sec:data-preprocessing}
TODO

\subsection{Data Cleaning}\label{subsec:data-cleaning}
TODO


\section{Different kinds of Data}\label{sec:different-kinds-of-data}
TODO

\subsection{Fundamental Data}\label{subsec:fundamental-data}
TODO

\subsection{Market Data}\label{subsec:market-data}
TODO

\subsection{Analytics Data}\label{subsec:analytics-data}
TODO

\subsection{Alternative Data}\label{subsec:alternative-data}
TODO


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Getting Ready Agent
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\chapter{Agent}\label{ch:agent}
TODO


\section{Training Agent}\label{sec:training-agent}
TODO


\section{Testing Agent}\label{sec:testing-agent}
TODO


\section{Benchmarks and Results}\label{sec:benchmarks-and-results}
TODO


\section{Backtesting}\label{sec:backtesting}
TODO


\section{Portfolio Performance}\label{sec:portfolio-performance}
TODO


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Contribution to Finrl-Meta
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\chapter{Contribution to Finrl-Meta}\label{ch:contribution-to-finrl-meta}
TODO


\section{1}\label{sec:1}
TODO

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Evaluation
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\chapter{Evaluation}\label{ch:evaluation}
TODO


\section{1}\label{sec:12}
TODO

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Conclusion
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\chapter{Conclusion}\label{ch:conclusion}
TODO
%\end{document}
