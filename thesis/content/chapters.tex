%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Introduction
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Introduction}\label{ch:introduction}

% TODO: Purpose of RL
% TODO: Purpose of MDP
% TODO

% Overview of what this thesis is about
The stock market is a wide and quickly changing environment.
A lot of people are interested in it and a lot of people are trying to make money from it.
How to make money from it in the long or short term?
This thesis will focus on long-term investing, also called Portfolio Allocation~\cite[p.~67--69]{rao-2022}.

% Main goal of this thesis
The goal is to do the right decision at the right time, which in the Stock market means, when and at what price we should buy this stock or another.
This is more complicated than it seems because we are emotionally
based, and not every person is able to make rational decisions.

% RL
Here come into play Artificial Intelligence (AI) and Reinforcement Learning (RL).
Reinforcement Learning (RL) is one of three types of Artificial Intelligence.

RL is learning what to do, how to map situations to actions, to maximize a numerical reward signal.
RL differs from supervised learning in that the agent is not told what to do,
but instead must discover for itself what actions yield the most
reward by trial and error~\cite[p.~1]{sutton-2018}.

Based on this rewards and losses, the agent improves its decision-making
to get more rewards and fewer losses in the future
in interaction with the environment.

% MC & MDP
The relationship between Markov Decision Process (MDP) and RL is
that RL problem could be well described im MDP\@.
And more precisely is problem described in MDP,
the better and faster could RL agent learn.

The base for MDPs is Markov Chain (MC),
which is a stochastic model describing a sequence of possible events in which the probability
of each event depends only on the state attained in the previous event.
MDPs from MCs differ from by adding actions and rewards.
In other words, MDPs become MCs with one action in a states and all rewards are same
for given state and action.

% Dynamic Programming
In Dynamic Programming, we can see two variants of Reinforcement Learning
Policy Iteration and Value Iteration.

% Why do I choose this topic
In my opinion, the stock market is a very interesting topic.
I love investing and what is called ''passive income''.
This topic was already covered by many people, but because of the
vision of getting rich, people tend to keep their solutions secret.

As Warren Buffett said, ''Be fearful when others are greedy and greedy when others are fearful.''

% Goal
The goal is to evaluate, benchmark and try to improve the current solutions of models
based on Reinforcement Learning and MDPs from AI4Finance-Foundation~\cite{https://doi.org/10.48550/arxiv.2111.03995}.
The goal is to make a model that will be able to make decisions in the stock market
in order to maximize the profit by allocating the portfolio.

% TODO: Thesis Structure

\if 0

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Preliminaries
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\chapter{Preliminaries}\label{ch:preliminaries}
TODO


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Reinforcement Learning
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\chapter{Reinforcement Learning}\label{ch:reinforcement-learning}
Reinforcement learning in general are used for decision-making.
In RL there is no supervisor, who teaches the agent what actions to take.

So it is obvious that agent get the result after the action is taken.

The Reinforcement Learning is now used e.q. in robotics, games, finance, etc\ldots


The purpose of this chapter is to introduce the reader to the basics of Reinforcement Learning.

Section \ref{sec:rl-introduction} introduces the reader to the basics of Reinforcement Learning.
Explain the terminology and the basic concepts of Reinforcement Learning.
Section \ref{sec:rl-introduction} introduces the reader to the basics of Reinforcement Learning.
Explain the terminology and the basic concepts of Reinforcement Learning.

A reward $R_t$ is a scalar feedback signal.
Reinforcement Learning is based on \textit{reqard hypothesis} that the agent's goal is to maximize the total reward.

The information which determining the next state are: current state, action, reward

The current state is the function of history.
$S_t = f(H_t)$

Even if envitonemt in current state $S_{t}^{e}$ is visible, it may contain irrelevant information.


\section{Introduction}\label{sec:rl-introduction}

\subsubsection{Reinforcement Learning related to Stock Market}\label{subsec:rl-introduction}
\begin{itemize}
    \item Sometimes the reward could have long term consequences.
    \item Rewards could be delayed.
    \item Sometimes it may be better to sacrifice short-term rewards for long-term gains.
\end{itemize}

\begin{itemize}
    \item observations
    \item actions
    \item rewards
\end{}

\textbf{foo}
\textrm{fff}


\begin{definition}
    \textit{history} is the sequence of observations, actions, and rewards that the agent has experienced.
\end{definition}

\fi


\section{Classical Reinforcement Learning}\label{sec:classical-reinforcement-learning}
TODO


\section{Parts of Reinforcement Learning}\label{sec:parts-of-reinforcement-learning}
TODO


\section{Functionalities of Reinforcement Learning}\label{sec:functionalities-of-reinforcement-learning}
TODO


\section{Reinforcement Learning Algorithms}\label{sec:reinforcement-learning-algorithms2}
TODO


\section{Deep Reinforcement Learning}\label{sec:deep-reinforcement-learning}
TODO

\subsection{Exploration vs. Exploitation}\label{subsec:exploration-vs.-exploitation}
TODO


\section{Reinforcement Learning Algorithms}\label{sec:reinforcement-learning-algorithms}
TODO: Describe used RL Algorithms

\subsection{Deep Reinforcement Learning Algorithms}\label{subsec:deep-reinforcement-learning-algorithms}
TODO

\subsubsection{PPO}
TODO

\subsubsection{SAC}
TODO

\subsubsection{TD3}
TODO

\subsubsection{DDPG}
TODO


\section{Existing solutions}\label{sec:existing-solutions}
TODO


\section{Neural Networks}\label{sec:neural-networks}
TODO


\section{Used Frameworks}\label{sec:used-frameworks}
TODO


\section{Markov Property}
This equation says that the future is independent of the past given the present.
$\probP[S_{t+1}|S_t] = \probP[S_{t+1}|S_1,\ldots,S_t]$


\section{Partially Observable Markov Decision Process (POMDP)}

\subsection{Partially Observable Environments}
It is the situation where the agent have the restricted access to the environment
and information provided by them are available only partially.

Agent must remember everything that happened in the past.
Own agent representation $S_t^d$.
Complete environment representation $S_t^a=H_t$.
Belief of environment state: $S_t^a=(\probP[S_t^e=s^1])$.


\section{Value Function}
Value function is the expected return starting from state $s$ and then following policy $\pi$.
$v_{\pi}(s)=\expectP[R_t+\gamma\,R_{t+1}+\gamma^2\,R_{t+2}+\ldots|S_t=s]$

Transition probability $P_{ss'}^{\pi}$ is the probability of transitioning from state $s$ to state $s'$ under policy $\pi$.

\[
    \mathcal{P}_{ss'}^a=\probP[S'=s'|S=s,A=a]
\]

Reward function $R(s,a,s')$ is the reward received when transitioning from state $s$ to state $s'$ under action $a$.
\[
    R_s^a=\expectP[R|S=s,A=s]
\]

\subsection{Value Based Agents}\label{subsec:value-based-agents}
Value Function only.
Value Function is the expected return starting from state $s$ and then following policy $\pi$.

\subsection{Policy Agents}\label{subsec:policy-agents}
Policy only.
Policy is the probability of taking action $a$ in state $s$.

\subsection{Actor-Critic Agents}\label{subsec:actor-critic-agents}
Policy + Value function


\section{Policy}
Policy is how agent behaves in the environment.
Map state to actions
Deterministic policy $a=\pi(s)$.
$\pi(a|s)=\probP[A=a|S=s]$

\subsection{Model Free}
Policy and/or Value Function.


\section{RL Agent Taxonomy}
% TODO: add image from: https://youtu.be/2pWv7GOvuf0?list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ&t=4549


\section{Sequential Decision Making}
Here two problems need to be solved:
In Reinforcement Learning:
\begin{itemize}
    \item The environment is initially unknown.
    \item The agent must learn to act in the environment.
    \item The agent improves its policy by interacting with the environment.
\end{itemize}

Planning:
\begin{itemize}
    \item A model of the environment is unknown.
    \item The agent performs computations with its model (without any external interaction).
    \item The agent improves its policy by performing computations with its model.
\end{itemize}

The planning ahead is needed to find the optimal policy, e.g. tree search.

All the time we are solving the problem of what will be the next state and what will be the reward.

Reinforcement Learning is like trial-and-error learning.


\section{Exploration vs. Exploitation}\label{sec:exploration-vs.-exploitation}
The time and costs is divided into two parts: Exploration and Exploitation.

The goal is to fidn the best trade-off between exploration and exploitation.

In the~\ref{subsec:exploration} the exploration approach is described and
in the~\ref{subsec:exploitation} the exploitation approach is described.

\subsection{Exploration}\label{subsec:exploration}
Finding new information about the environment.

\subsection{Exploitation}\label{subsec:exploitation}
Exploiting the information that is already known to maximize the reward.


\section{Prediction and Control}\label{sec:prediction-vs.-control}
Here to solve the control problem we need to solve the prediction problem first.
In the~\ref{subsec:exploration} the exploration approach is described and
in the~\ref{subsec:exploitation} the exploitation approach is described.

\subsection{Prediction}\label{subsec:prediction}
Given a policy.

\subsection{Control}\label{subsec:control}
Finding the best policy.


\section{Markov Processes (also called Markov Chains)}\label{sec:markov-processes}
Markov Process is a stochastic process where the future is independent of the past given the present.

\begin{definition}
    A Markov process is a tuple $(S,A,P)$ where:
    \begin{itemize}
        \item $S$ is a finite set of states.
        \item $A$ is a finite set of actions.
        \item $P$ is a transition probability function fot time $t$, $P_t:S\times A\times S\to[0,1]$.
    \end{itemize}
\end{definition}
$\probP[S_{t+1}|S_t] = \probP[S_{t+1}|S_1,\ldots,S_t]$


\section{Markov Reward Process (MRP)}\label{sec:markov-reward-process}
Markov Reward Process is a Markov Process with a reward function.
MRP is a tuple of $g\in G\subseteq L \times \mathrm{Ops} \times L$
\begin{definition}
    A Markov Reward Process is a Markov Process, along with a time-indexed
    sequence of Reward random variables $R_t \in D$ (a countable subset of R) for time steps $t =
    1,2,\ldots$, satisfying the Markov Property (including Rewards):
    $\probP[(R_{t+1}, S_{t+1})|S_t, S_{t-1}, \ldots, S_0] = P[(Rt+1, St+1)|St]$ for all $t \leq 0.$~\cite[p.~79]{rao-2022}
\end{definition}

The total reward discounted reward from time-step $t$ is given by:
\[
    G_t = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}
\]
for $\gamma \in [0,1]$.

\subsection{Why discount factor $\gamma$ is needed?}
Because there is more uncertainty in the future than in the past.
So it is basicaly used to balance the future and the past, because
model is not always perfect.

So, Why?
\begin{itemize}
    \item Mathematical convenient to discount rewards.
    \item To prevent rewards blow-up to infinity
    \item Uncertainty about the future may not be fully captured by the model.
    \item In finance: Immediate rewards are more important than future rewards.
    (the current value of money is generally greater than in a year, because of inflation)
    \item It is sometimes possible to use $\gamma=1$, e.q.\ when all sequences terminate.
\end{itemize}

The terminal state is the state where the episode ends, that means there
is no more transitions from that state to any other state.

The point in $G_t$ is that it tells us how good the state is, because
it is the sum of all the rewards from that state to the end of the episode. % TODO: true?


\section{Dynamic Programming vs. Reinforcement Learning}\label{sec:dynamic-programming-vs.-reinforcement-learning}
Dynamic Programming is a method for solving Markov Decision Processes (MDP).
The agent assumes that the transition probabilities and rewards are known.
The Dynamic Programming Algorithm are considered to be planning and not learning,
because the algorithm doesn’t need to interact with the Environment and
doesn’t need to learn from the (states, rewards) data stream coming
from the Environment~\cite[p.~28]{rao-2022}.


\section{Markov Decision Process}\label{sec:markov-decision-process}
Formally describe an environment for reinforcement learning.

The current state $S_t$ completely characterizes the process. % TODO: is it correct?
Almost all RL problems can be described as MDPs.
Optimal control primarily deals with continous MDPs.
Into MSPs the POMDPs could be converted.

\[
    \probP[S_{t+1}|S_t]=\probP[S_{t+1}|S_1,\ldots,S_t]
\]

State transition matrix.
\[
    \mathcal{P}_{ss'}^a=\probP[S'=s'|S=s,A=a]
\]

where $S'$ is the all next possible states and $s'$ is the next concrete state from $S'$.

\[
    \mathcal{P} = \begin{bmatrix}
                      \mathcal{P}_{11} & \dots  & \mathcal{P}_{1n} \\
                      \vdots           & \ddots & \vdots           \\
                      \mathcal{P}_{11} & \dots  & \mathcal{P}_{nn}
    \end{bmatrix}
\]
where each row of the matrix sum to 1.

\subsection{Bellman Equation}\label{subsec:bellman-equation}
The value function can be decomposed into the immediate reward $R_{t+1}$
and the discounted value of the next state $\gamma\ v(S_{t+1})$,
where $v$ is the value function.

\begin{eqnarray*}
    v(s) = &=& \mathbb{E}[G_{t}|S_t=s]\\
    &=& \mathbb{E}[R_{t+1} + \gamma\,R_{t+2}+\gamma^2\,R_{t+3}+\ldots|S_t=s]\\
    &=& \mathbb{E}[R_{t+1} + \gamma(R_{t+2}+\gamma\,R_{t+3}+\ldots)|S_t=s]\\
    &=& \mathbb{E}[R_{t+1} + \gamma\,G_{t+1}|S_t=s]\\
    &=& \mathbb{E}[R_{t+1} + \gamma\,v(S_{t+1})|S_t=s]
\end{eqnarray*}

\begin{center}
    \begin{tikzpicture}
        [auto,node distance=3mm,>=latex,font=\small]
        \tikzstyle{round}=[thick,draw=black,circle]

        % Nodes
        \node[round, label=left:$\upsilon(s)\mapsfrom\,s$] (s0) {};
        \node[round,below left=10mm and 5mm of s0] (s1) {};
        \node[round,label={[shift={(-2.1,0.0)}]left:$\upsilon(s')\mapsfrom\,s'$},below right=10mm and 5mm of s0] (s2) {};

        % Paths
        \draw[-] (s0) -- (s1) node[midway,left] {$r$};
        \draw[-] (s0) -- (s2);
    \end{tikzpicture}
\end{center}

\[
    \text{v(s)} = \mathcal{R}_{s} + \gamma\,\sum_{s'\in\mathcal{S}}^{}\mathcal{P}_{ss'}\,v(s')
\]

\subsection{Bellman Optimality Equation}\label{subsec:bellman-optimality-equation}
TODO

\begin{eqnarray*}
    v(s) &=& \mathcal{R}+\gamma\,\mathcal{P}\text{v}\\
    (I - \gamma\,\mathcal{P})\,\text{v} &=& \mathcal{R}\\
    \text{v} &=& (I-\gamma\mathcal{P})^{-1} R\\
\end{eqnarray*}
But the computation complexity of the inverse matrix is $O(n^3)$ for n states.
So for big MRPs the inverse matrix method is not feasible.
So the more usefull method is the iterative method:
\begin{itemize}
    \item Dynamic Programming
    \item Monte Carlo evaluation
    \item Temporal Difference learning
\end{itemize}


\section{Markov Decision Process}\label{sec:markov-decision-process2}
\begin{definition}
    A Markov Decision Process (MDP) is a tuple $\mathcal{M}=(\mathcal{S},\mathcal{A},\mathcal{P},\mathcal{R},\gamma)$
    where
    $\mathcal{S}$ is the finite set of states, \\
    $\mathcal{A}$ is the finite set of actions, \\
    $\mathcal{P}$ is the state transition probability matrix,
    $\mathcal{P}_{ss'}^{a}=\mathbb{P}[S_{t+1}=s'|S_t=s,A_t=a]$\\
    $\mathcal{R}$ is the reward function, $\mathcal{R}_{s}^{a}=\mathbb{E}[R_{t+1}|S_t=s,A_t=a]$ \\
    and  \\
    $\gamma$ is the discount factor $\gamma\in[0,1]$. \\
\end{definition} % TODO: citation

\subsection{Policy}\label{subsec:policy}
Policies are distributions over actions given states.
\[
    \pi(a|s) = \mathbb{P}[A_t=a|S_t=s]
\]
where MDPs only depend on the current state and action (not history).
So the policy is a function of the current state and defines
the behavior of the agent.
Policies are time-independent, so $A_t=\pi(\cdot|S_t),\forall\,t>0$.


If we evaluate a Markov Decision Process (MDP)
$\mathcal{M}=\langle\mathcal{S},\mathcal{A},\mathcal{P},\mathcal{R},\gamma\rangle$
with a fixed policy $\pi$ (in general, with a fixed stochastic policy $\pi$), we get the
Markov Reward Process (MRP) $\langle \mathcal{S},\mathcal{P}^{\pi},\mathcal{R}^{\pi},\gamma\rangle$.
that is implied by the combination of the MDP and the
policy $\pi$~\cite[100]{rao-2022}.

where
\begin{align}
    \mathcal{P}^{\pi}_{ss'} &= \sum_{a\in\mathcal{A}}^{}\pi(a|s)\,\mathcal{P}_{ss'}^{a} \\
    \mathcal{R}^{\pi}_{s} &= \sum_{a\in\mathcal{A}}^{}\pi(a|s)\,\mathcal{R}_{s}^{a}
\end{align}

\subsection{Value Function}\label{subsec:value-function}

\subsubsection{State-Value Function}\label{subsubsec:state-value-function}
The value function of a policy $\pi$ is defined as
\[
    \text{v}_{\pi}(s) = \mathbb{E}_{\pi}[G_t|S_t=s]
\]
where $G_t$ is the total discounted reward starting from time $t$.
The value function of a policy $\pi$ is the expected return starting from state $s$ and following policy $\pi$.

\subsubsection{Action-Value Function}\label{subsubsec:action-value-function}
The action-value function of a policy $\pi$ is defined as
\[
    \text{q}_{\pi}(s,a) = \mathbb{E}_{\pi}[G_t|S_t=s,A_t=a]
\]
where $G_t$ is the total discounted reward starting from time $t$.
The action-value function of a policy $\pi$ is the expected return starting from state $s$ and following policy $\pi$.


\section{Bellman Expectation Equation}\label{sec:bellman-expectation-equation}
The Bellman Expectation Equation is a recursive equation that describes the value function of a policy $\pi$.

value function of a policy $\pi$:
\[
    \text{v}_{\pi}^{s}=\mathbb{E}[R_{t+1}\gamma\text{v}_{\pi}(S_{t+1})|S_t=s]
\]
The equation says, how much reward can be obtain by following policy $\pi$ starting from state $s$.

adn the action-value function of a policy $\pi$:
\[
    \text{q}_{\pi}^{s,a}=\mathbb{E}[R_{t+1}\gamma\text{q}_{\pi}(S_{t+1},A_{t+1})|S_t=s,A_t=a]
\]

Let consider the following graph, the black nodes are the
actions and the white nodes are the states.
\begin{center}
    \begin{tikzpicture}
        [auto,>=latex,font=\small]
        \tikzstyle{round}=[thick,draw=black,circle,node distance=3mm]
        \tikzstyle{round_black}=[thick,draw=black,circle,fill=black,node distance=2mm]

        % Nodes
        \node[round, label=left:$\upsilon(s)\mapsfrom\,s$] (s0) {};
        \node[round_black,below left=10mm and 5mm of s0] (s1) {};
        \node[round_black,label={[shift={(-1.5,0.0)}]left:$q_{\pi}(s,a)\mapsfrom\,a$},below right=10mm and 5mm of s0] (s2) {}; % TODO: better label positioning

        % Paths
        \draw[-] (s0) -- (s1) node[midway,left] {$r$};
        \draw[-] (s0) -- (s2);
    \end{tikzpicture}
\end{center}

% TODO: from 55:00

<<<<<<< Updated upstream
=======
\subsection{Optimal Value Function}\label{subsec:optimal-value-function}
The optimal state-value function $v_{*}(s)$ is the maximum value function over all policies.
\[
    v_{*}(s) = \max_{\pi}\text{v}_{\pi}(s)
\]

The optimal action-value function $q_{*}(s,a)$ is the maximum action-value function over all policies.
\[
    q_{*}(s,a) = \max_{\pi}\text{q}_{\pi}(s,a)
\]

\subsection{Partial Ordering of Policies}\label{subsec:partial-ordering-of-policies}
Value functions define a partial ordering over policies.

\begin{definition}[Partial Ordering of Policies]
    \label{def:partial-ordering-of-policies}
    Let $\pi_1$ and $\pi_2$ be two policies.
    Then $\pi_1\leq\pi_2$ if and only if $\text{v}_{\pi_1}(s)\geq\text{v}_{\pi_2}(s),\forall\,s\in\mathcal{S}$.
    There is always one policy that is better than all other policies.
    \[
        \pi_1\leq\pi_2\iff\text{v}_{\pi_1}(s)\leq\text{v}_{\pi_2}(s),\forall\,s\in\mathcal{S}
    \]
    This is an optimal policy $\pi_{*}$.
    The optimal policy $\pi_{*}$ is the policy that maximizes the action-value function.
    $q_{\pi_{*}}(s,a) = q_*(s,a)$
\end{definition}\cite[p.~84]{sutton-2018}

Optimal policy can be found by maximizing over $q_*(s,a)$
\[
    \pi_{*}(a|s) =
    \begin{cases}
        1 & \text{if}\,a=\arg\max_{a\in\mathcal{A}}q_*(s,a) \\
        0 & \text{otherwise}
    \end{cases}
\]


\section{Bellman Optimality Equation}\label{sec:bellman-optimality-equation}
The Bellman Optimality Equation is a recursive equation that describes the optimal value function $v_*(s)$.

\newcommand{\tikzAngleOfLine}{\tikz@AngleOfLine}
\def\tikz@AngleOfLine(#1)(#2)#3{%
    \pgfmathanglebetweenpoints{%
        \pgfpointanchor{#1}{center}}{%
        \pgfpointanchor{#2}{center}}
    \pgfmathsetmacro{#3}{\pgfmathresult}%
}


\begin{definition}
    The optimal value function $v_*(s)$ is the maximum value function over all actions
    we can take in state $s$.
    \[
        v_*(s) = \max_{a}q_*(s,a)
    \]
\end{definition}
\begin{center}
    \begin{tikzpicture}
%        \useasboundingbox rectangle (2,2);
        [auto,>=latex,font=\small]
        \tikzstyle{round}=[thick,draw=black,circle,minimum size=3mm]
        \tikzstyle{round_black}=[thick,draw=black,circle,fill=black,scale=0.6]

        % Nodes
        \node[round, label=left:$v(s)\mapsfrom\,s$] (s0) {};
        \node[round_black,below left=10mm and 5mm of s0] (s1) {};
        \node[round_black,label={[shift={(-1.6,0.0)}]left:$q(s,a)\mapsfrom\,a$},below right=10mm and 5mm of s0] (s2) {};

        % Paths
        \draw[-] (s0) -- (s1) node[midway,left] {};
        \draw[-] (s0) -- (s2);

        % Angle
        \tikzAngleOfLine(s0)(s2){\AngleStart}
        \tikzAngleOfLine(s0)(s1){\AngleEnd}
        \draw[black,-] (s0)+(\AngleStart:0.6cm) arc (\AngleStart:\AngleEnd:0.6cm);
    \end{tikzpicture}
\end{center}


\begin{center}
    \begin{tikzpicture}
%        \useasboundingbox rectangle (2,2);
        [auto,>=latex,font=\small]
        \tikzstyle{round}=[thick,draw=black,circle,minimum size=3mm]
        \tikzstyle{round_black}=[thick,draw=black,circle,fill=black,scale=0.6]

        % Nodes
%        \node[round_black, label=left:$q_*(s,a)\mapsfrom\,s,a$] (s0) {};
        \node[round_black, label=left:$q_{*}({s,a})\mapsfrom\,{s,a}$] (s0) {};
        \node[round,below left=10mm and 5mm of s0] (s1) {};
        \node[round,label={[shift={(-1.6,0.0)}]left:$v_*(s')\mapsfrom\,s'$},below right=10mm and 5mm of s0] (s2) {};

        % Paths
        \draw[-] (s0) -- (s1) node[midway,left] {$r$};
        \draw[-] (s0) -- (s2);

        % Angle
        \tikzAngleOfLine(s0)(s2){\AngleStart}
        \tikzAngleOfLine(s0)(s1){\AngleEnd}
        \draw[black,-] (s0)+(\AngleStart:0.6cm) arc (\AngleStart:\AngleEnd:0.6cm);
    \end{tikzpicture}
\end{center}

If the optimal value function $v_*(s')$ is known, then we can choose the
action $a$ that maximizes the action-value function $q_*(s,a)$ by
averaging over the probability of taking action $a$ in state $s$ and end
up in state $s'$ and the immediate reward $R$ taking action $a$ in state
$s$.

This tells us how good are the actions we can take in state $s$.


\section{Bellman Optimality Equation for $V^*$}\label{sec:bellman-optimality-equation-for-v}
\[
    q_*(s,a) = \mathcal{R}_{s}^{a} + \gamma\sum_{s'\in\,S}\mathcal{P}_{ss'}^{a}v_*(s')
\]

\begin{definition}
    The Bellman Optimality Equation is the recursive equation that describes the optimal value function $v_*(s)$.
    \[
        v_*(s) = \max_{a}\left(\mathcal{R}_{s}^{a} + \gamma\sum_{s'\in\,S}\mathcal{P}_{ss'}^{a}v_*(s')\right)
    \]
\end{definition}

\begin{tikzpicture}
    [auto,>=latex,font=\small]
    \tikzstyle{round}=[thick,draw=black,circle,minimum size=3mm]
    \tikzstyle{round_black}=[thick,draw=black,circle,fill=black,scale=0.6]

    % 1 layer
    \node[round, label=left:$v_*(s)\mapsfrom\,s$] (s0) {};

    % 2 layer
    \node[round_black,below left=10mm and 8mm of s0]                                        (s1) {};
    \node[round_black,below right=10mm and 8mm of s0, label={[shift={(-2.1,0.0)}]left:$a$}] (s2) {};

    % Paths
    \draw[-] (s0) -- (s1);
    \draw[-] (s0) -- (s2);

    % Angle
    \tikzAngleOfLine(s0)(s2){\AngleStart}
    \tikzAngleOfLine(s0)(s1){\AngleEnd}
    \draw[black,-] (s0)+(\AngleStart:0.6cm) arc (\AngleStart:\AngleEnd:0.6cm);

    % 3 layer
    \node[round,below left=10mm and 5mm of s1, label={[shift={(0.0,0.0)}]left:$v_*(s')\mapsfrom\,s'$}]  (s3) {};
    \node[round,below right=10mm and 5mm of s1]                                       (s4) {};
    \node[round,below left=10mm and 5mm of s2]                                        (s5) {};
    \node[round,below right=10mm and 5mm of s2]                                       (s6) {};

    % Paths
    \draw[-] (s1) -- (s3) node[midway,left] {$r$};
    \draw[-] (s1) -- (s4);

    \draw[-] (s2) -- (s5);
    \draw[-] (s2) -- (s6);

\end{tikzpicture}

\[
    v_*(s) = \max_{a}\left(\mathcal{R}_{s}^{a} + \gamma\sum_{s'\in\,S}\mathcal{P}_{ss'}^{a}v_*(s')\right)
\]


\section{Bellman Optimality Equation for $Q^*$}\label{sec:bellman-optimality-equation-for-q}
\[
    q_*(s,a) = \mathcal{R}_{s}^{a} + \gamma\sum_{s'\in\,S}\mathcal{P}_{ss'}^{a}v_*(s')
\]

\begin{tikzpicture}
    [auto,>=latex,font=\small]
    \tikzstyle{round}=[thick,draw=black,circle,minimum size=3mm]
    \tikzstyle{round_black}=[thick,draw=black,circle,fill=black,scale=0.6]

    % 1 layer
    \node[round, label=left:$q_*({s,a})\mapsfrom\,{s,a}$] (s0) {};

    % 2 layer
    \node[round_black,below left=10mm and 8mm of s0]                                        (s1) {$r$};
    \node[round_black,below right=10mm and 8mm of s0, label={[shift={(-2.1,0.0)}]left:$s'$}] (s2) {};

    % Paths
    \draw[-] (s0) -- (s1);
    \draw[-] (s0) -- (s2);

    % Angle
    \tikzAngleOfLine(s0)(s2){\AngleStart}
    \tikzAngleOfLine(s0)(s1){\AngleEnd}
    \draw[black,-] (s0)+(\AngleStart:0.6cm) arc (\AngleStart:\AngleEnd:0.6cm);

    % 3 layer
    \node[round,below left=10mm and 5mm of s1, label={[shift={(0.0,0.0)}]left:$q_*({s',a'})\mapsfrom\,a'$}]  (s3) {};
    \node[round,below right=10mm and 5mm of s1]                                       (s4) {};
    \node[round,below left=10mm and 5mm of s2]                                        (s5) {};
    \node[round,below right=10mm and 5mm of s2]                                       (s6) {};

    % Paths
    \draw[-] (s1) -- (s3) node[midway,left] {};
    \draw[-] (s1) -- (s4);

    \draw[-] (s2) -- (s5);
    \draw[-] (s2) -- (s6);
\end{tikzpicture}

\[
    q_*(s,a) = \mathcal{R}_{s}^{a} + \gamma\sum_{s'\in\,S}\mathcal{P}_{ss'}^{a}v_*(s')
\]

Bellman Optimality Equation is non-linear and non-convex.
It is not possible to solve it analytically.
However, it is possible to solve it numerically using iterative methods.
\begin{itemize}
    \item Value Iteration
    \item Policy Iteration
    \item Q-Learning
    \item SARSA
    \item \ldots
\end{itemize}


\section{Planning by Dynamic Programming}\label{sec:planning-by-dynamic-programming}
Dynamic programming is a method for solving complex problems by breaking them down into simpler subproblems.
It is a general technique for solving optimization problems.
It is a mathematical programming method that can be used to find an optimal policy for a given Markov decision process (MDP).
Dynamic programming assumes that the MDP is known and finite.
It is used for planning in an MDP.
For prediction:
\begin{itemize}
    \item \textbf{Input}: MDP $\mathcal{M} = \langle\,S,\,A,\,R,\,P,\,\gamma\,\rangle$ and policy $\pi$
    or: MDP $\mathcal{M} = \langle\,S,\,A,\,R,\,P,\,\gamma\,\rangle$ and value function $v_*$ or $q_*$.
    \item \textbf{Output}: Value function $v_*$ or $q_*$.
\end{itemize}

for control:
\begin{itemize}
    \item \textbf{Input}: MDP $\mathcal{M} = \langle\,S,\,A,\,R,\,P,\,\gamma\,\rangle$ and value function $v_*$ or $q_*$.
    \item \textbf{Output}: Optimal value function $v_*$ and optimal policy $\pi_*$.
\end{itemize}


\section{Where to stop learning?}\label{sec:where-to-stop-learning}
% TODO:
$\epsilon$-convergence.

\subsection{Bellman Expectation Equation}\label{sec:bellman-expectation-equation}
Problem is ot find optimal policy $\pi_*$.

\subsection{States backups}
TODO
% TODO:

\subsection{Value Iteration}\label{sec:value-iteration}
One solution is to use Bellman Optimality backup.
$v_1\rightarrow\,v_2\ldots\rightarrow\,v_*$

\[
    v_{k+1}(s) = \max_{a\in\,A}\left(\mathcal{R}_{s}^{a} + \gamma\sum_{s'\in\,S}\mathcal{P}_{ss'}^{a}v_k(s')\right)
\]

% TODO: Should be here this equation?
\[
    v_*(s) = \mathbb{E}_{\pi}\left[\sum_{t=0}^{\infty}\gamma^tR_{t+1}\,\bigg|\,S_0=s\right]
\]


\begin{center}
    \begin{NiceTabular}{ |wl{2cm}|wl{7cm}|wc{3.5cm}|  }
        \hline
        \multicolumn{3}{|c|}{\textbf{Synchronous Dynamic Programming Algorithms}} \\
        \hline
        \RowStyle[color=red]{}
        \textbf{Problem} & \textbf{Bellman Equation}                                           & \textbf{Algorithm}          \\
        \hline
        Prediction       & Bellman Expectation Equation                                        & \makecell{Iterative \\ Policy Evaluation} \\
        \hline
        Control          & \makecell{Bellman Expectation Equation \\ + Greedy Policy Improvements} & Policy Iteration            \\
        \hline
        Control          & Bellman Optimality Equation                                         & Value Iteration             \\
        \hline
    \end{NiceTabular}
\end{center}

Synchronous value iteration stores two copies of the value function.
for all $s\in\,S$:
\[
    v_{new}(s) \leftarrow \max_{a\in\,A}\left(\mathcal{R}_{s}^{a} + \gamma\sum_{s'\in\,S}\mathcal{P}_{ss'}^{a}v_{old}(s')\right)
\]

\[
    v_{old}(s) \leftarrow v_{new}(s)
\]

In-place value iteration stores only one copy of the value function.
for all $s\in\,S$:
\[
    v(s) \leftarrow \max_{a\in\,A}\left(\mathcal{R}_{s}^{a} + \gamma\sum_{s'\in\,S}\mathcal{P}_{ss'}^{a}v(s')\right)
\]

So the ordering really matters.
The efficiency of the algorithm depends on the ordering of value calculation and value update
and can be really improved by using a better ordering.

Use magnitude of Bellman error to guide state selection, e.g.
% TODO: The Absolute values should be with or without max function
\begin{gather*}
    \Delta v(s) = \max_{a\in\,A}\left|\mathcal{R}_{s}^{a} + \gamma\sum_{s'\in\,S}\mathcal{P}_{ss'}^{a}v(s') - v(s)\right|\\
    \Delta v(s) = \left|\max_{a\in\,A}\mathcal{R}_{s}^{a} + \gamma\sum_{s'\in\,S}\mathcal{P}_{ss'}^{a}v(s') - v(s)\right|\\
\end{gather*}


\section{Model Free Prediction}\label{sec:model-free-prediction}

\subsection{Introduction}\label{subsec:introduction}

\subsection{Monte-Carlo Learning}\label{subsec:monte-carlo-learning}
MC learns directly from episodes of experience.
MC is model free, that means it does not need a model of the environment
like MDPs (transition probabilities and rewards).
MC learns from complete episodes of experience (no bootstrapping).
Caveat: MC is off-policy, that means it can learn from any policy, not only the optimal one. % TODO: Is this true?
All episodes must terminate.

Goal is to learn the value function $v_*$, from episodes of experience under policy $\pi$.
$S_1, A_1, R_2, \ldots, S_k \sim\,\pi$

$G_t$ is the total reward from time $t$ to the end of the episode.
\[
    G_t = \sum_{k=t}^{\infty}\gamma^{k-t}R_k
\]

$v_{\pi}(s)$ is the value function, representing the expected return starting from state $s$ and following policy $\pi$.
\[
    v_{\pi}(s) = \mathbb{E}_{\pi}\left[G_1\,\bigg|\,S_1=s\right]
\]

\subsubsection{First-visit MC}\label{subsubsec:first-visit-mc}
We are generating episodes of experience under policy $\pi$.

The first time-step $t$ that state $s$ is visited:
\begin{itemize}
    \item Increment the number of times state $s$ has been visited: $N(s) \leftarrow N(s) + 1$.
    \item Update the value of state $s$: $v(s) \leftarrow v(s) + \frac{1}{N(s)}\left(G_t - v(s)\right)$.
\end{itemize}
N(s): How many time is state $s$ visited in the episode for the first time.
S(s): Total return over many episodes for state $s$.

Value is estimated as the average of returns when the state is visited for the first time.
\[
    v_{\pi}(s) = \frac{S(s)}{N(s)}
\]
By law of large numbers, $V(s)\rightarrow\,v_{\pi}(s)$ as $N(s)\rightarrow\infty$

\subsubsection{Every-visit MC}\label{subsubsec:every-visit-mc}
Every time-step $t$ that state $s$ is visited:
\begin{itemize}
    \item Increment the number of times state $s$ has been visited: $N(s) \leftarrow N(s) + 1$.
    \item Increment the total return when state $s$ is visited: $S(s) \leftarrow S(s) + G_t$.
\end{itemize}
Value is estimated as the average of returns when the state is visited.
\[
    v_{\pi}(s) = \frac{S(s)}{N(s)}
\]
And again by law of large numbers, $V(s)\rightarrow\,v_{\pi}(s)$ as $N(s)\rightarrow\infty$





Monte-Carlo policy evaluation uses empirical average of return instead of the expected return.

\subsection{Temporal-Difference Learning}\label{subsec:temporal-difference-learning}

\subsection{TD($\lambda$) Learning}\label{subsec:td-lambda-learning}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Experiments
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

>>>>>>> Stashed changes

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Exisiting Approaches
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\chapter{Existing Approaches to Portfolio Allocation}\label{ch:existing-approaches-to-portfolio-allocation}
TODO


\section{Introduction}\label{sec:introduction}
TODO


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Environment
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\chapter{Environment}\label{ch:environment}
TODO


\section{Stock Market Environment}\label{sec:stock-market-environment}
TODO

\subsection{Used Frameworks}\label{subsec:used-frameworks}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Data
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\chapter{Data Engineering}\label{ch:data-engineering}
TODO


\section{Data Collection}\label{sec:data-collection}
TODO


\section{Data Preprocessing}\label{sec:data-preprocessing}
TODO

\subsection{Data Cleaning}\label{subsec:data-cleaning}
TODO


\section{Different kinds of Data}\label{sec:different-kinds-of-data}
TODO

\subsection{Fundamental Data}\label{subsec:fundamental-data}
TODO

\subsection{Market Data}\label{subsec:market-data}
TODO

\subsection{Analytics Data}\label{subsec:analytics-data}
TODO

\subsection{Alternative Data}\label{subsec:alternative-data}
TODO


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Getting Ready Agent
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\chapter{Agent}\label{ch:agent}
TODO


\section{Training Agent}\label{sec:training-agent}
TODO


\section{Testing Agent}\label{sec:testing-agent}
TODO


\section{Benchmarks and Results}\label{sec:benchmarks-and-results}
TODO


\section{Backtesting}\label{sec:backtesting}
TODO


\section{Portfolio Performance}\label{sec:portfolio-performance}
TODO


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Contribution to Finrl-Meta
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\chapter{Contribution to Finrl-Meta}\label{ch:contribution-to-finrl-meta}
TODO


\section{1}\label{sec:1}
TODO

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Evaluation
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\chapter{Evaluation}\label{ch:evaluation}
TODO


\section{1}\label{sec:12}
TODO

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Conclusion
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\chapter{Conclusion}\label{ch:conclusion}
TODO
