%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Introduction
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Introduction}\label{ch:introduction}

% TODO: Purpose of RL
% TODO: Purpose of MDP
% TODO

% Overview of what this thesis is about
The stock market is a wide and quickly changing environment.
A lot of people are interested in it and a lot of people are trying to make money from it.
How to make money from it in the long or short term?
This thesis will focus on long-term investing, also called Portfolio Allocation~\cite[p.~67--69]{rao-2022}.

% Main goal of this thesis
The goal is to do the right decision at the right time, which in the Stock market means, when and at what price we should buy this stock or another.
This is more complicated than it seems because we are emotionally
based, and not every person is able to make rational decisions.

% RL
Here come into play Artificial Intelligence (AI) and Reinforcement Learning (RL).
Reinforcement Learning (RL) is one of three types of Artificial Intelligence.

RL is learning what to do, how to map situations to actions, to maximize a numerical reward signal.
RL differs from supervised learning in that the agent is not told what to do,
but instead must discover for itself what actions yield the most
reward by trial and error~\cite[p.~1]{sutton-2018}.

Based on this rewards and losses, the agent improves its decision-making
to get more rewards and fewer losses in the future
in interaction with the environment.

% MC & MDP
The relationship between Markov Decision Process (MDP) and RL is
that RL problem could be well described im MDP\@.
And more precisely is problem described in MDP,
the better and faster could RL agent learn.

The base for MDPs is Markov Chain (MC),
which is a stochastic model describing a sequence of possible events in which the probability
of each event depends only on the state attained in the previous event.
MDPs from MCs differ from by adding actions and rewards.
In other words, MDPs become MCs with one action in a states and all rewards are same
for given state and action.

% Dynamic Programming
In Dynamic Programming, we can see two variants of Reinforcement Learning
Policy Iteration and Value Iteration.

% Why do I choose this topic
In my opinion, the stock market is a very interesting topic.
I love investing and what is called ''passive income''.
This topic was already covered by many people, but because of the
vision of getting rich, people tend to keep their solutions secret.

As Warren Buffett said, ''Be fearful when others are greedy and greedy when others are fearful.''

% Goal
The goal is to evaluate, benchmark and try to improve the current solutions of models
based on Reinforcement Learning and MDPs from AI4Finance-Foundation~\cite{https://doi.org/10.48550/arxiv.2111.03995}.
The goal is to make a model that will be able to make decisions in the stock market
in order to maximize the profit by allocating the portfolio.

% TODO: Thesis Structure

\if 0

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Preliminaries
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\chapter{Preliminaries}\label{ch:preliminaries}
TODO


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Reinforcement Learning
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\chapter{Reinforcement Learning}\label{ch:reinforcement-learning}
Reinforcement learning in general are used for decision-making.
In RL there is no supervisor, who teaches the agent what actions to take.

So it is obvious that agent get the result after the action is taken.

The Reinforcement Learning is now used e.q. in robotics, games, finance, etc\ldots


The purpose of this chapter is to introduce the reader to the basics of Reinforcement Learning.

Section \ref{sec:rl-introduction} introduces the reader to the basics of Reinforcement Learning.
Explain the terminology and the basic concepts of Reinforcement Learning.
Section \ref{sec:rl-introduction} introduces the reader to the basics of Reinforcement Learning.
Explain the terminology and the basic concepts of Reinforcement Learning.

A reward $R_t$ is a scalar feedback signal.
Reinforcement Learning is based on \textit{reqard hypothesis} that the agent's goal is to maximize the total reward.

The information which determining the next state are: current state, action, reward

The current state is the function of history.
$S_t = f(H_t)$

Even if envitonemt in current state $S_{t}^{e}$ is visible, it may contain irrelevant information.


\section{Introduction}\label{sec:rl-introduction}

\subsubsection{Reinforcement Learning related to Stock Market}\label{subsec:rl-introduction}
\begin{itemize}
    \item Sometimes the reward could have long term consequences.
    \item Rewards could be delayed.
    \item Sometimes it may be better to sacrifice short-term rewards for long-term gains.
\end{itemize}

\begin{itemize}
    \item observations
    \item actions
    \item rewards
\end{}

\textbf{foo}
\textrm{fff}


\begin{definition}
    \textit{history} is the sequence of observations, actions, and rewards that the agent has experienced.
\end{definition}

\fi


\section{Classical Reinforcement Learning}\label{sec:classical-reinforcement-learning}
TODO


\section{Parts of Reinforcement Learning}\label{sec:parts-of-reinforcement-learning}
TODO


\section{Functionalities of Reinforcement Learning}\label{sec:functionalities-of-reinforcement-learning}
TODO


\section{Reinforcement Learning Algorithms}\label{sec:reinforcement-learning-algorithms2}
TODO


\section{Deep Reinforcement Learning}\label{sec:deep-reinforcement-learning}
TODO

\subsection{Exploration vs. Exploitation}\label{subsec:exploration-vs.-exploitation}
TODO


\section{Reinforcement Learning Algorithms}\label{sec:reinforcement-learning-algorithms}
TODO: Describe used RL Algorithms

\subsection{Deep Reinforcement Learning Algorithms}\label{subsec:deep-reinforcement-learning-algorithms}
TODO

\subsubsection{PPO}
TODO

\subsubsection{SAC}
TODO

\subsubsection{TD3}
TODO

\subsubsection{DDPG}
TODO


\section{Existing solutions}\label{sec:existing-solutions}
TODO


\section{Neural Networks}\label{sec:neural-networks}
TODO


\section{Used Frameworks}\label{sec:used-frameworks}
TODO


\section{Markov Property}
This equation says that the future is independent of the past given the present.
$\probP[S_{t+1}|S_t] = \probP[S_{t+1}|S_1,\ldots,S_t]$


\section{Partially Observable Markov Decision Process (POMDP)}

\subsection{Partially Observable Environments}
It is the situation where the agent have the restricted access to the environment
and information provided by them are available only partially.

Agent must remember everything that happened in the past.
Own agent representation $S_t^d$.
Complete environment representation $S_t^a=H_t$.
Belief of environment state: $S_t^a=(\probP[S_t^e=s^1])$.


\section{Markov Processes}\label{sec:markov-processes}
TODO


\section{Markov Decision Process}\label{sec:markov-decision-process}
TODO

\subsection{Bellman Equation}\label{subsec:bellman-equation}
TODO

\subsection{Bellman Optimality Equation}\label{subsec:bellman-optimality-equation}
TODO


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Exisiting Approaches
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\chapter{Existing Approaches to Portfolio Allocation}\label{ch:existing-approaches-to-portfolio-allocation}
TODO


\section{Introduction}\label{sec:introduction}
TODO


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Environment
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\chapter{Environment}\label{ch:environment}
TODO


\section{Stock Market Environment}\label{sec:stock-market-environment}
TODO

\subsection{Used Frameworks}\label{subsec:used-frameworks}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Data
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\chapter{Data Engineering}\label{ch:data-engineering}
TODO


\section{Data Collection}\label{sec:data-collection}
TODO


\section{Data Preprocessing}\label{sec:data-preprocessing}
TODO

\subsection{Data Cleaning}\label{subsec:data-cleaning}
TODO


\section{Different kinds of Data}\label{sec:different-kinds-of-data}
TODO

\subsection{Fundamental Data}\label{subsec:fundamental-data}
TODO

\subsection{Market Data}\label{subsec:market-data}
TODO

\subsection{Analytics Data}\label{subsec:analytics-data}
TODO

\subsection{Alternative Data}\label{subsec:alternative-data}
TODO


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Getting Ready Agent
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\chapter{Agent}\label{ch:agent}
TODO


\section{Training Agent}\label{sec:training-agent}
TODO


\section{Testing Agent}\label{sec:testing-agent}
TODO


\section{Benchmarks and Results}\label{sec:benchmarks-and-results}
TODO


\section{Backtesting}\label{sec:backtesting}
TODO


\section{Portfolio Performance}\label{sec:portfolio-performance}
TODO


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Contribution to Finrl-Meta
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\chapter{Contribution to Finrl-Meta}\label{ch:contribution-to-finrl-meta}
TODO


\section{1}\label{sec:1}
TODO

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Evaluation
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\chapter{Evaluation}\label{ch:evaluation}
TODO


\section{1}\label{sec:12}
TODO

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Conclusion
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\chapter{Conclusion}\label{ch:conclusion}
TODO
